{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "wTXgwreyIgD0",
    "outputId": "ff582b4c-dd98-4f3c-ae8b-a6c1850f3d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.4)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.13.19)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.16.19)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.19->boto3->pytorch_pretrained_bert) (1.12.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: barbar in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install barbar\n",
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "F6eouRsg6x8J",
    "outputId": "f13e35e1-8c3b-4ba9-f34d-aa800c50d012"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content\n",
      "['.config', 'gdrive', 'sample_data']\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "root_dir = \"/content/gdrive/My Drive/\"\n",
    "base_dir = root_dir + 'gpt2data/'\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(base_dir)\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "e6I5pj0yH-2X",
    "outputId": "f792d9ff-354e-4bf9-e453-8ee7e6401c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "from barbar import Bar\n",
    "import math\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from itertools import chain\n",
    "import json\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import cached_path\n",
    "import tarfile\n",
    "import tempfile\n",
    "import logging\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise GPT-2 Model and tokenizer with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5Il8hMMOMq39",
    "outputId": "0991e127-4d61-4552-b591-9fbe621c7da8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2DoubleHeadsModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50262, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
       "  (multiple_choice_head): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (activation): Identity()\n",
       "    (first_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (last_dropout): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
    "\n",
    "# Add special tokens to embeddings and vocabulary of the model\n",
    "SPECIAL_TOKENS = ['<BOS>', '<EOS>', '<speaker1>', '<speaker2>', '<PAD>']\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>',\n",
    "                       'additional_special_tokens': ['<speaker1>', '<speaker2>']}  # ,'speaker2' : '<speaker2>']}\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "aJzPIqC2N9kw",
    "outputId": "defe5e0c-e1c5-446e-b229-a344ae967bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14118 14118\n",
      "96117 96117\n",
      "[75396, 1756, 674, 491, 12842, 1598, 3360]\n",
      "10519\n",
      "[2000, 1756, 674, 491, 2000, 1598, 2000]\n",
      "10519\n",
      "10519\n"
     ]
    }
   ],
   "source": [
    "datafile = 'dialogues_text2.txt'\n",
    "emodatafile = 'dialogues_emotion2.txt'\n",
    "\n",
    "bos, eos, speaker1, speaker2, CLS  = \"<BOS>\", \"<EOS>\", \"<speaker1>\", \"<speaker2>\", \"<CLS>\"\n",
    "\n",
    "def normalizeString(s): #normailizes string\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def filtertomaxlength(pair,emotionpair): #makes the max string have 200 words\n",
    "    pair1 = []\n",
    "    pair2 = []\n",
    "    for i in range(len(pair)):\n",
    "        length = 0\n",
    "        for j in range(len(pair[i])):\n",
    "            length += len(pair[i][j].split(' '))\n",
    "        \n",
    "        if length < 51:\n",
    "            pair1.append(pair[i])\n",
    "            pair2.append(emotionpair[i])\n",
    "    \n",
    "    return pair1,pair2\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_dialogues2(datafile,emodatafile): #takes text and puts it into a list \n",
    "    emopairs = []\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "            read().strip().split('\\n')\n",
    "            \n",
    "    emolines = open(emodatafile, encoding='utf-8').\\\n",
    "            read().strip().split('\\n')\n",
    "        # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('__eou__')] for l in lines]\n",
    "    for i in range(len(pairs)):\n",
    "        pairs[i] = pairs[i][:-1] #removes empty character and keeps rest\n",
    "        emopairs.append(emolines[i].split(' '))\n",
    "        emopairs[-1] = emopairs[-1][:-1]\n",
    "        if len(emopairs[i]) != len(pairs[i]):\n",
    "            print(i)\n",
    "            print(len(emopairs[i]))\n",
    "            print(len(pairs[i]))\n",
    "            print(emopairs[i])\n",
    "            print(pairs[i])\n",
    "    return pairs,emopairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createdata(pairs,emotionpairs):\n",
    "    output = []\n",
    "    emotions = []\n",
    "    for line in range(len(pairs)):        \n",
    "        for j in range(len(pairs[line])-1):\n",
    "            if (len(tokenizer.encode(pairs[line][j])) + len(tokenizer.encode(pairs[line][j+1]))) <= 51:\n",
    "                output.append([pairs[line][j],pairs[line][j+1]])\n",
    "                emotions.append([emotionpairs[line][j],emotionpairs[line][j+1]])\n",
    "    return output,emotions\n",
    "\n",
    "\n",
    "def balancedata(pairs,emotionpairs, maxbal):\n",
    "    distribution = [0,0,0,0,0,0,0]\n",
    "\n",
    "    for labely in range(7):\n",
    "        for count in emotionpairs:\n",
    "            #print(count)\n",
    "            #print(labely)\n",
    "            if int(count[1]) == labely:\n",
    "                #print('yes')\n",
    "                distribution[labely] += 1\n",
    "    \n",
    "    print(distribution)\n",
    "\n",
    "\n",
    "    newpairs = []\n",
    "    newemotions = []\n",
    "    distribution = [0,0,0,0,0,0,0]\n",
    "\n",
    "    for count in range(len(emotionpairs)):\n",
    "        if distribution[int(emotionpairs[count][1])] < maxbal:\n",
    "                distribution[int(emotionpairs[count][1])] += 1\n",
    "                newpairs.append(pairs[count])\n",
    "                newemotions.append(emotionpairs[count])\n",
    "    \n",
    "    print(len(newemotions))\n",
    "\n",
    "\n",
    "    distribution = [0,0,0,0,0,0,0]\n",
    "    \n",
    "    for labely in range(7):\n",
    "        for count in newemotions:\n",
    "            #print(count)\n",
    "            #print(labely)\n",
    "            if int(count[1]) == labely:\n",
    "                #print('yes')\n",
    "                distribution[labely] += 1\n",
    "    \n",
    "    print(distribution)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return newpairs,newemotions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pairs,emotionpairs = create_dialogues2(datafile,emodatafile)\n",
    "\n",
    "print(len(pairs), len(emotionpairs))\n",
    "\n",
    "pairs,emotionpairs = createdata(pairs,emotionpairs)\n",
    "\n",
    "\n",
    "print(len(pairs), len(emotionpairs))\n",
    "\n",
    "pairs,emotionpairs  = balancedata(pairs,emotionpairs,2000)\n",
    "\n",
    "print(len(emotionpairs))\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adds vocab not in GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T7AwZs8gN-ph",
    "outputId": "e51a7a49-4ae1-46ad-accd-93bf6082059b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(55805, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this block adds vocab no in GPT2 to gpt2\n",
    "vocab = []\n",
    "for x in range(len(pairs)):\n",
    "    for j in range(len(pairs[x])):\n",
    "        words = pairs[x][j].split(' ')\n",
    "        for w in words:\n",
    "            if w not in vocab:\n",
    "                vocab.append(w)\n",
    "tokenizer.add_tokens(vocab)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to get data into Persona, History, Reply format along with Input_IDs, Token_Type_IDs,MC_Token IDs, Language Modelling and Multiple Choice labels,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLoIzHQROBg5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build Inputs and Create_rest_of_dataset change a question answer pair into appropriate format to feed into model\n",
    "def build_inputs(persona, history, reply):\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence) - i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))  # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1  # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))  # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "\n",
    "def create_instance(personality, history, reply):\n",
    "    # print('histor in create rest of dataset', history)\n",
    "    seq1 = []\n",
    "    for i in history:\n",
    "        seq1.append(re.sub(\"[^\\w]\", \" \", i).split())\n",
    "\n",
    "    seq0 = personality\n",
    "\n",
    "    seq2 = re.sub(\"[^\\w]\", \" \", reply).split()\n",
    "    words, segments, position, sequence = build_inputs(seq0, seq1, seq2)\n",
    "\n",
    "    # gold reply\n",
    "\n",
    "    words = tokenizer.convert_tokens_to_ids(words)\n",
    "    segments = tokenizer.convert_tokens_to_ids(segments)\n",
    "\n",
    "    lm_targets = ([-100] * sum(len(s) for s in sequence[:-1])) \\\n",
    "                 + [-100] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "    last_token = len(words) - 1\n",
    "\n",
    "    # distractor creation\n",
    "\n",
    "    words_distractor_list = []\n",
    "    segments_distractor_list = []\n",
    "    lm_distractor_list = []\n",
    "    last_token_distractor_list = []\n",
    "\n",
    "    emotionscovered = [0, 0, 0, 0, 0, 0, 0]\n",
    "    emotionscovered[int(personality)] = 1\n",
    "\n",
    "    for numdistractors in range(7):\n",
    "        check = 1\n",
    "        while check == 1:\n",
    "            distractor = random.choice(range(len(pairs)))\n",
    "            if emotionscovered[numdistractors] == 0:\n",
    "                if int(emotionpairs[distractor][-1]) == numdistractors:\n",
    "                    emotionscovered[numdistractors] = 1\n",
    "                    check = 0\n",
    "                    distractor = pairs[distractor][-1]\n",
    "\n",
    "                    distractor = re.sub(\"[^\\w]\", \" \", distractor).split()\n",
    "                    words_distractor, segments_distractor, _, _ = build_inputs(seq0, seq1, distractor)\n",
    "                    words_distractor = tokenizer.convert_tokens_to_ids(words_distractor)\n",
    "                    segments_distractor = tokenizer.convert_tokens_to_ids(segments_distractor)\n",
    "                    lm_distractor = [-100] * len(words_distractor)\n",
    "                    last_token_distractor = len(words_distractor) - 1\n",
    "\n",
    "                    words_distractor_list.append(words_distractor)\n",
    "                    segments_distractor_list.append(segments_distractor)\n",
    "                    lm_distractor_list.append(lm_distractor)\n",
    "                    last_token_distractor_list.append(last_token_distractor)\n",
    "            else:\n",
    "                check = 0\n",
    "\n",
    "    padding_length = 100\n",
    "\n",
    "    (words, segments) = [pad(x, tokenizer.convert_tokens_to_ids('<PAD>'), padding_length)\n",
    "                         for x in (words, segments)]\n",
    "\n",
    "    for m in range(len(words_distractor_list)):\n",
    "        (words_distractor_list[m], segments_distractor_list[m]) = [\n",
    "            pad(x, tokenizer.convert_tokens_to_ids('<PAD>'), padding_length)\n",
    "            for x in (words_distractor_list[m], segments_distractor_list[m])]\n",
    "        lm_distractor_list[m] = pad(lm_distractor_list[m], -100, padding_length)\n",
    "\n",
    "    lm_targets = pad(lm_targets, -100, padding_length)\n",
    "\n",
    "    ###########################\n",
    "    inputs_list = [words]\n",
    "    tokentype_list = [segments]\n",
    "    mctoken_list = [last_token]\n",
    "    lmlabel_list = [lm_targets]\n",
    "\n",
    "    for numcand in range(6):\n",
    "        inputs_list.append(words_distractor_list[numcand])\n",
    "        tokentype_list.append(segments_distractor_list[numcand])\n",
    "        mctoken_list.append(last_token_distractor_list[numcand])\n",
    "        lmlabel_list.append(lm_distractor_list[numcand])\n",
    "\n",
    "    input_ids = torch.tensor(inputs_list, dtype=torch.long)\n",
    "    token_type_ids = torch.tensor(tokentype_list, dtype=torch.long)\n",
    "    mc_token_ids = torch.tensor(mctoken_list, dtype=torch.long)\n",
    "    lm_labels = torch.tensor(lmlabel_list, dtype=torch.long)\n",
    "    mc_labels = torch.tensor([0], dtype=torch.long)\n",
    "\n",
    "    return input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels\n",
    "\n",
    "\n",
    "# Evaluation data tokenizer\n",
    "\n",
    "def build_input_from_segments(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence) - i) % 2 else speaker1] + s for i, s in\n",
    "                                enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n",
    "\n",
    "\n",
    "def pad(x, padding, padding_length):\n",
    "    return x + [padding] * (padding_length - len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataloader for train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLGguOePOE1L"
   },
   "outputs": [],
   "source": [
    "class Pairsdataset(Dataset):\n",
    "    def __init__(self, pairs,emotionpairs):\n",
    "        \n",
    "        self.pairs = pairs\n",
    "        self.emotionpairs = emotionpairs\n",
    "        \n",
    "        self.valuelist = []\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.pairs)):\n",
    "            #print(i)\n",
    "            instance = create_instance(self.emotionpairs[i][-1],self.pairs[i][:-1],self.pairs[i][-1])\n",
    "            self.valuelist.append(instance)\n",
    "                \n",
    "             \n",
    "        maxl = 0\n",
    "        \n",
    "        for i in range(len(self.valuelist)):\n",
    "            \n",
    "            if len(self.valuelist[i][0][0]) > maxl:\n",
    "                maxl = len(self.valuelist[i][0][0])\n",
    "                f = i\n",
    "        print(maxl,f)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "      \n",
    "        input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels = self.valuelist[i]\n",
    "        return input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jUegw6UTOH28",
    "outputId": "323c7adf-f6bc-4411-918e-51dde02892ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0\n",
      "519\n"
     ]
    }
   ],
   "source": [
    "Train_dataset = Pairsdataset(pairs,emotionpairs)\n",
    "\n",
    "train, valid,test = torch.utils.data.random_split(Train_dataset, [9000,1000,519])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train, batch_size= 2, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid, batch_size= 2, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test, batch_size= 1, shuffle=False, num_workers=0)\n",
    "\n",
    "#Creating testing dataset\n",
    "\n",
    "test_list = []\n",
    "for x in range(len(test)):\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for i in range(len(test[x][0][0])):\n",
    "        \n",
    "        if test[x][0][0][i] == 50261:\n",
    "            s = i + 1\n",
    "        if test[x][0][0][i] == 50260:\n",
    "            e = i\n",
    "    \n",
    "    tokens = test[x][0][0][s:e]\n",
    "    \n",
    "    \n",
    "    for chars in tokens:\n",
    "        sentence = sentence + (tokenizer.convert_ids_to_tokens([chars])[0]) + ' ' \n",
    "        \n",
    "        \n",
    "    test_list.append(sentence[:-1])\n",
    "\n",
    "print(len(test_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up optimizer and training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cOiuyS9MOKpx",
    "outputId": "c49dfea6-c09f-4095-ec2d-19d6c9e126e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 6.25e-5\n",
    "optimizer = AdamW(model.parameters(), lr=6.25e-5, correct_bias=True)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "lm_coef = 1.0\n",
    "mc_coef = 1.0\n",
    "\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oTHD6799ONgp",
    "outputId": "5e81e930-d2e8-4640-dfa3-d52ba63c38a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "9000/9000: [===============================>] - ETA 33.7s\n",
      "Epoch number 1\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "9000/9000: [===============================>] - ETA 33.5s\n",
      "Epoch number 2\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "1000/1000: [===============================>] - ETA 0.1s\n",
      "9000/9000: [===============================>] - ETA 33.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'gpt2_model/'\n",
    "def train_batch():\n",
    "    grad_acc = 8\n",
    "    lm_coef = 1.0\n",
    "    mc_coef = 1.0\n",
    "    minloss = 100\n",
    "    loss_analytics = []\n",
    "    lm_loss_analytics = []\n",
    "    mc_loss_analytics = []\n",
    "    valloss_analytics = []\n",
    "    current_loss_analytics = []\n",
    "    current_lm_loss_analytics = []\n",
    "    current_mc_loss_analytics = []\n",
    "    perplexity = []\n",
    "    vallm_loss_analytics = []\n",
    "    valmc_loss_analytics = []\n",
    "    valnll_analytics = []\n",
    "    iterations = []\n",
    "    output_dir = 'gpt2_model/'\n",
    "    total_loss = 0\n",
    "    total_lmloss = 0\n",
    "    total_mcloss = 0\n",
    "    total_val_loss = 0\n",
    "    total_val_lmloss = 0\n",
    "    total_val_mcloss = 0\n",
    "    for i in range(epochs):        \n",
    "        print('Epoch number', i)\n",
    "        for id, X in enumerate(Bar(train_loader)):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels = X\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            # print(input_ids)\n",
    "            # print(token_type_ids)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            mc_token_ids = mc_token_ids.to(device)\n",
    "            lm_labels = lm_labels.to(device)\n",
    "            mc_labels = mc_labels.to(device)\n",
    "\n",
    "            (lm_loss), (mc_loss), *_ = model(\n",
    "            input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids,\n",
    "            mc_labels=mc_labels, lm_labels=lm_labels)\n",
    "\n",
    "            loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "            current_loss_analytics.append(loss)\n",
    "            current_lm_loss_analytics.append(lm_loss)\n",
    "            current_mc_loss_analytics.append(mc_loss)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_lmloss += lm_loss.item()\n",
    "            total_mcloss += mc_loss.item()\n",
    "            iteration = (i * len(train_loader)) + id + 1\n",
    "            loss_analytics.append(total_loss / iteration)\n",
    "            lm_loss_analytics.append(total_lmloss / iteration)\n",
    "            mc_loss_analytics.append(total_mcloss / iteration)\n",
    "           \n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            _ = nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            if id % 200 == 0:\n",
    "                iterations.append(iteration)\n",
    "                total_val_loss = 0\n",
    "                total_val_lmloss = 0\n",
    "                total_val_mcloss = 0\n",
    "                model.eval()\n",
    "                for id, batch in enumerate(Bar(valid_loader)):\n",
    "                    input_ids, token_type_ids, mc_token_ids, lm_labels, mc_labels= batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    # print(input_ids)\n",
    "                    # print(token_type_ids)\n",
    "                    token_type_ids = token_type_ids.to(device)\n",
    "                    mc_token_ids = mc_token_ids.to(device)\n",
    "                    lm_labels = lm_labels.to(device)\n",
    "                    mc_labels = mc_labels.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        (lm_loss), (mc_loss), *_ = model(input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids, mc_labels=mc_labels, lm_labels=lm_labels)\n",
    "                        loss = lm_loss * lm_coef + mc_loss * mc_coef\n",
    "                        total_val_loss += loss.item()\n",
    "                        total_val_lmloss += lm_loss.item()\n",
    "                        total_val_mcloss += mc_loss.item()\n",
    "                perplexity.append(math.exp(total_val_lmloss/len(valid_loader)))        \n",
    "                vallm_loss_analytics.append(total_val_lmloss / len(valid_loader))\n",
    "                valmc_loss_analytics.append(total_val_mcloss / len(valid_loader))\n",
    "                valloss_analytics.append(total_val_loss / len(valid_loader))\n",
    "    with open('loss_analytics', 'wb') as f:\n",
    "        pickle.dump(loss_analytics, f)\n",
    "    with open('lm_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(lm_loss_analytics, f)\n",
    "    with open('mc_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(mc_loss_analytics, f)\n",
    "    with open('valloss_analytics', 'wb') as f:\n",
    "        pickle.dump(valloss_analytics, f)\n",
    "    with open('vallm_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(vallm_loss_analytics, f)\n",
    "    with open('valmc_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(valmc_loss_analytics, f)\n",
    "    with open('current_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(current_loss_analytics, f)\n",
    "    with open('current_lm_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(current_lm_loss_analytics, f)\n",
    "    with open('current_mc_loss_analytics', 'wb') as f:\n",
    "        pickle.dump(valmc_loss_analytics, f)\n",
    "    with open('iterations', 'wb') as f:\n",
    "        pickle.dump(iterations, f)\n",
    "    model.save_pretrained(save_directory=output_dir)\n",
    "    tokenizer.save_pretrained(save_directory=output_dir)\n",
    "    \n",
    "\n",
    "train_batch()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Model using Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Rynnl_USKX5T"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "history = []\n",
    "reply = \"\"\n",
    "first = 1\n",
    "sent = \"\"\n",
    "\n",
    "\n",
    "temperature = 0.7\n",
    "decoding_strategy = \"nucleus\" # choose between greedy, beam, top-k, nucleus\n",
    "\n",
    "replies = []\n",
    "prompts = []\n",
    "\n",
    "\n",
    "for i in test_list:\n",
    "        \n",
    "        #initialize bert \n",
    "        tokenizer = BertTokenizer.from_pretrained('/Sentiment Analysis/')\n",
    "        model = BertForSequenceClassification.from_pretrained('./Sentiment Analysis/',num_labels = 5, output_attentions = False, output_hidden_states = False)\n",
    "        model.to(device)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if first !=1:\n",
    "            #This is for getting the emotion of our previous reply\n",
    "            ############################\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 78,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "            \n",
    "            \n",
    "            input_ids = [encoded_dict['input_ids']]\n",
    "            attention_masks = [encoded_dict['attention_mask']]\n",
    "\n",
    "            input_ids = torch.cat(input_ids, dim=0)\n",
    "            attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_masks.to(device)\n",
    "\n",
    "            outputs = model(input_ids,token_type_ids=None,attention_mask = attention_mask)\n",
    "            values, indices = torch.max(outputs[0][0],0)\n",
    "\n",
    "            personality = indices.item()\n",
    "            #print('Reply personality', personality, personalities[personality])\n",
    "            \n",
    "            replies.append(personality)\n",
    "        ###############################\n",
    "        first = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        #get our input\n",
    "        \n",
    "        raw_text = i\n",
    "        \n",
    "        \n",
    "        # This is for getting the emotion of our input\n",
    "        ############################\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        raw_text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 78,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "        \n",
    "        input_ids = [encoded_dict['input_ids']]\n",
    "        attention_masks = [encoded_dict['attention_mask']]\n",
    "        \n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_masks.to(device)\n",
    "        \n",
    "        outputs = model(input_ids,token_type_ids=None,attention_mask = attention_mask)\n",
    "        values, indices = torch.max(outputs[0][0],0)\n",
    "        \n",
    "        personality = indices.item()\n",
    "        \n",
    "        \n",
    "        prompts.append(personality)\n",
    "        \n",
    "        #Shifts emotions from BERT to be same as trained for GPT-2\n",
    "        if personality > 1:\n",
    "            personality += 2\n",
    "        \n",
    "               \n",
    "        #############################\n",
    "        \n",
    "        #Change to GPT-2 model, language modelling head only\n",
    "        model = GPT2LMHeadModel.from_pretrained(output_dir).to(device)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "        \n",
    "       \n",
    "        seq0 = str(personality)\n",
    "        seq1 = re.sub(\"[^\\w]\", \" \",  raw_text).split()\n",
    "        seq2 = re.sub(\"[^\\w]\", \" \",  reply).split()\n",
    "        \n",
    "        history.append(re.sub(\"[^\\w]\", \" \",  raw_text).split())\n",
    "        #Writes reply according to decoding strategy\n",
    "        while True:\n",
    "\n",
    "            instance =  build_input_from_segments(seq0,history,seq2, tokenizer, lm_labels=False, with_eos= False)\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(instance[\"input_ids\"])\n",
    "\n",
    "            token_type_ids = tokenizer.convert_tokens_to_ids(instance[\"token_type_ids\"])\n",
    "\n",
    "            input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "            \n",
    "            token_type_ids = torch.tensor([token_type_ids], dtype=torch.long)\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "             \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, token_type_ids = token_type_ids)\n",
    "                \n",
    "                \n",
    "                \n",
    "                predictions = outputs[0]\n",
    "                \n",
    "               \n",
    "                ################\n",
    "                #Changes based on decoding strategy \n",
    "                \n",
    "                if decoding_strategy == \"greedy\":\n",
    "                    value,greedy = torch.max(predictions[0, -1, :],0)\n",
    "                    prev = greedy\n",
    "    \n",
    "                elif decoding_strategy == \"top-k\":\n",
    "            \n",
    "                    predictions = predictions[0, -1, :]/ temperature\n",
    "                    predictions = top_filtering(predictions, top_k=20, top_p= 0.0)\n",
    "                    probs = F.softmax(predictions, dim = -1)\n",
    "                    prev = torch.multinomial(probs, 1)\n",
    "                    \n",
    "    \n",
    "                elif decoding_strategy == \"nucleus\":\n",
    "                \n",
    "                    predictions = predictions[0, -1, :]/ temperature\n",
    "                    predictions = top_filtering(predictions, top_k=0, top_p=0.9)\n",
    "                    probs = F.softmax(predictions, dim = -1)\n",
    "                    prev = torch.multinomial(probs, 1)   \n",
    "                    \n",
    "                \n",
    "                predicted_text = tokenizer.decode([prev.item()])\n",
    "                if predicted_text in SPECIAL_TOKENS:\n",
    "                    x = 1\n",
    "                else:\n",
    "                    seq2.append(predicted_text)\n",
    "                    \n",
    "                    \n",
    "            if predicted_text == '<EOS>' and len(seq2)>0:\n",
    "                history.append(seq2)\n",
    "                break\n",
    "        \n",
    "                \n",
    "        \n",
    "        history = []\n",
    "        sent = \"\"\n",
    "        for i in range(len(seq2)):\n",
    "            sent = sent + str(seq2[i]) + ' '\n",
    "        \n",
    "         \n",
    "#Gets confusion matrix\n",
    "prompts = prompts[:-1]\n",
    "cm = confusion_matrix(prompts, replies)\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "totalcorrects = 0\n",
    "\n",
    "\n",
    "for i in range(len(replies)):\n",
    "    if replies[i] == prompts[i]:\n",
    "        totalcorrects += 1\n",
    "\n",
    "        \n",
    "print(cmn)\n",
    "print(totalcorrects/(len(replies)))  \n",
    "print(confusion_matrix(prompts, replies))\n",
    "sn.heatmap(cmn,cmap = 'Reds', annot=True)  \n",
    "plt.ylabel('Detected Emotion of Prompt')\n",
    "plt.xlabel('Detected Emotion of Reply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75npJQxoP354"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GPT2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
