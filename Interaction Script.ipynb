{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "from barbar import Bar\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel, AdamW, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import cached_path\n",
    "import tarfile\n",
    "import tempfile\n",
    "import logging\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_from_segments(persona,history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]\n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence))\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
    "    return instance\n",
    "\n",
    "\n",
    "bos, eos, speaker1, speaker2, CLS  = \"<BOS>\", \"<EOS>\", \"<speaker1>\", \"<speaker2>\", \"<CLS>\"\n",
    "SPECIAL_TOKENS = ['<BOS>', '<EOS>', '<speaker1>', '<speaker2>', '<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i am very hungry and i do not feel well\n",
      "Input personality 3 I am sad.\n",
      "i am so sorry i feel well i am very sick \n",
      "Reply personality 3 I am sad.\n",
      ">>> i am sorry that you feel sick. Can i help you?\n",
      "Input personality 3 I am sad.\n",
      "i know i know what you need to do i ll be back soon \n",
      "Reply personality 3 I am sad.\n",
      ">>> ok, i will be waiting for you here\n",
      "Input personality 2 I am happy.\n",
      "i will be right \n",
      "Reply personality 3 I am sad.\n",
      ">>> can not wait\n",
      "Input personality 2 I am happy.\n",
      "how much \n",
      "Reply personality 0 I have no emotion.\n",
      ">>> a lot\n",
      "Input personality 3 I am sad.\n",
      "i know \n",
      "Reply personality 3 I am sad.\n",
      ">>> good see you soon\n",
      "Input personality 2 I am happy.\n",
      "i will be back \n",
      "Reply personality 2 I am happy.\n",
      ">>> bye \n",
      "Input personality 2 I am happy.\n",
      "i will be right \n",
      "Reply personality 3 I am sad.\n",
      ">>> q\n",
      "[['i', 'am', 'very', 'hungry', 'and', 'i', 'do', 'not', 'feel', 'well'], ['i', 'am', 'so', 'sorry', 'i', 'feel', 'well', 'i', 'am', 'very', 'sick'], ['i', 'am', 'sorry', 'that', 'you', 'feel', 'sick', 'Can', 'i', 'help', 'you'], ['i', 'know', 'i', 'know', 'what', 'you', 'need', 'to', 'do', 'i', 'll', 'be', 'back', 'soon'], ['ok', 'i', 'will', 'be', 'waiting', 'for', 'you', 'here'], ['i', 'will', 'be', 'right'], ['can', 'not', 'wait'], ['how', 'much'], ['a', 'lot'], ['i', 'know'], ['good', 'see', 'you', 'soon'], ['i', 'will', 'be', 'back'], ['bye'], ['i', 'will', 'be', 'right']]\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "reply = \"\"\n",
    "first = 1\n",
    "sent = \"\"\n",
    "\n",
    "personalities = [\"I have no emotion.\", \"I am angry.\", \"I am happy.\", \"I am sad.\", \"I am surprised.\"]\n",
    "\n",
    "temperature = 0.7\n",
    "decoding_strategy = \"nucleus\" # choose between greedy, beam, top-k, nucleus\n",
    "while True:\n",
    "        \n",
    "        #initialize bert \n",
    "        \n",
    "        tokenizer = BertTokenizer.from_pretrained('./Sentiment Analysis/')\n",
    "        model = BertForSequenceClassification.from_pretrained('./Sentiment Analysis/',num_labels = 5, output_attentions = False, output_hidden_states = False)\n",
    "        model.to(device)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if first !=1:\n",
    "            #This is for getting the emotion of our previous reply\n",
    "            ############################\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 78,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "            \n",
    "            \n",
    "            input_ids = [encoded_dict['input_ids']]\n",
    "            attention_masks = [encoded_dict['attention_mask']]\n",
    "\n",
    "            input_ids = torch.cat(input_ids, dim=0)\n",
    "            attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_masks.to(device)\n",
    "\n",
    "            outputs = model(input_ids,token_type_ids=None,attention_mask = attention_mask)\n",
    "            values, indices = torch.max(outputs[0][0],0)\n",
    "\n",
    "            personality = indices.item()\n",
    "            #print('Reply personality', personality)\n",
    "            print('Reply personality',personality, personalities[personality])\n",
    "        \n",
    "        ###############################\n",
    "        first = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        #get our input\n",
    "        raw_text = input(\">>> \")\n",
    "        if raw_text == 'q': \n",
    "                break\n",
    "        while not raw_text:\n",
    "            print('Prompt should not be empty!')\n",
    "            raw_text = input(\">>> \")\n",
    "            if raw_text == 'q': \n",
    "                break\n",
    "\n",
    "               \n",
    "    \n",
    "\n",
    "        # This is for getting the emotion of our input\n",
    "        ############################\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        raw_text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 78,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "        \n",
    "        input_ids = [encoded_dict['input_ids']]\n",
    "        attention_masks = [encoded_dict['attention_mask']]\n",
    "        \n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_masks.to(device)\n",
    "        \n",
    "        outputs = model(input_ids,token_type_ids=None,attention_mask = attention_mask)\n",
    "        values, indices = torch.max(outputs[0][0],0)\n",
    "        \n",
    "                                                           \n",
    "        \n",
    "                                                           \n",
    "        personality = indices.item()\n",
    "        print('Input personality',personality, personalities[personality])  \n",
    "        if personality > 1:\n",
    "            personality += 2 \n",
    "        #############################\n",
    "        \n",
    "        \n",
    "        model = GPT2LMHeadModel.from_pretrained('./gpt2_model/').to(device)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('./gpt2_model/')\n",
    "        \n",
    "        \n",
    "        seq0 = str(personality) \n",
    "        seq1 = re.sub(\"[^\\w]\", \" \",  raw_text).split()\n",
    "        seq2 = re.sub(\"[^\\w]\", \" \",  reply).split()\n",
    "        \n",
    "        history.append(re.sub(\"[^\\w]\", \" \",  raw_text).split())\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            instance =  build_input_from_segments(seq0,history,seq2, tokenizer, lm_labels=False, with_eos= False)\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(instance[\"input_ids\"])\n",
    "\n",
    "            token_type_ids = tokenizer.convert_tokens_to_ids(instance[\"token_type_ids\"])\n",
    "\n",
    "            input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "            \n",
    "            token_type_ids = torch.tensor([token_type_ids], dtype=torch.long)\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "           \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, token_type_ids = token_type_ids)\n",
    "                \n",
    "                \n",
    "                \n",
    "                predictions = outputs[0]\n",
    "               \n",
    "                ################\n",
    "                #Changes based on decoding strategy \n",
    "                \n",
    "                if decoding_strategy == \"greedy\":\n",
    "                    value,greedy = torch.max(predictions[0, -1, :],0)\n",
    "                    prev = greedy\n",
    "    \n",
    "                elif decoding_strategy == \"top-k\":\n",
    "            \n",
    "                    predictions = predictions[0, -1, :]/ temperature\n",
    "                    predictions = top_filtering(predictions, top_k=20, top_p= 0.0)\n",
    "                    probs = F.softmax(predictions, dim = -1)\n",
    "                    prev = torch.multinomial(probs, 1)\n",
    "                    \n",
    "    \n",
    "                elif decoding_strategy == \"nucleus\":\n",
    "                \n",
    "                    predictions = predictions[0, -1, :]/ temperature\n",
    "                    predictions = top_filtering(predictions, top_k=0, top_p=0.9)\n",
    "                    probs = F.softmax(predictions, dim = -1)\n",
    "                    prev = torch.multinomial(probs, 1)   \n",
    "                    \n",
    "                \n",
    "                predicted_text = tokenizer.decode([prev.item()])\n",
    "                if predicted_text in SPECIAL_TOKENS:\n",
    "                    x = 1\n",
    "                else:\n",
    "                    seq2.append(predicted_text)\n",
    "                    \n",
    "                    \n",
    "            if predicted_text == '<EOS>' and len(seq2)>0:\n",
    "                history.append(seq2)\n",
    "                break\n",
    "        \n",
    "                \n",
    "        #history = history[-(2):]\n",
    "        sent = \"\"\n",
    "        for i in range(len(seq2)):\n",
    "            sent = sent + str(seq2[i]) + ' '\n",
    "        print(sent)\n",
    "         \n",
    "print(history)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
